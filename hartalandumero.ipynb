{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime, timedelta\n",
    "import selenium.webdriver.common.keys\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the ChromeDriver executable and start a Chrome browser using Selenium\n",
    "driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the webpage\n",
    "driver.get('https://merolagani.com/Floorsheet.aspx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract data from the current page\n",
    "def extract_page_data():\n",
    "    # Let's use BeautifulSoup to parse the page source\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract the data you need from the current page\n",
    "    # Adjust the selectors based on your HTML structure\n",
    "    data = []\n",
    "    rows = soup.select('.table-bordered tbody tr')\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        row_data = [column.get_text(strip=True) for column in columns]\n",
    "        data.append(row_data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the \"Items per page\" filter is a dropdown with the class 'ng-untouched'\n",
    "# Wait for the dropdown to be present on the page\n",
    "#wait = WebDriverWait(driver, 10)\n",
    "#items_per_page_dropdown = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'ng-untouched')))\n",
    "#items_per_page_dropdown.click()\n",
    "# Find the option with value '500' and click on it\n",
    "#option_500 = wait.until(EC.element_to_be_clickable((By.XPATH, '//option[@value=\"500\"]')))\n",
    "#option_500.click()\n",
    "#filter_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'box__filter--search')))\n",
    "#filter_button.click()\n",
    "#print(driver.page_source)\n",
    "#driver.implicitly_wait(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01/17/2024', '01/18/2024', '01/19/2024', '01/20/2024', '01/21/2024']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the specific date (e.g., start from one week ago)\n",
    "start_date_str = '01/17/2024'\n",
    "start_date = datetime.strptime(start_date_str , '%m/%d/%Y')\n",
    "date_array = []\n",
    "\n",
    "\n",
    "# Get the current date\n",
    "end_date = datetime.now()\n",
    "\n",
    "# Iterate through the date range\n",
    "while start_date <= end_date:\n",
    "    # Format the current date as MM/DD/YYYY and print\n",
    "    current_date_str = start_date.strftime('%m/%d/%Y')\n",
    "#     date_input.clear()\n",
    "#     date_input.send.keys(current_date_str)\n",
    "    date_array.append(  current_date_str )\n",
    "     \n",
    "    # Move to the next date\n",
    "    start_date += timedelta(days=1)\n",
    "print(date_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to click the next page button\n",
    "def click_next_page():\n",
    "    try:\n",
    "        # Find the next page element and click it\n",
    "        next_page = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[text()=\"Next\"]'))\n",
    "        )\n",
    "        next_page.click()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking next page: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data from 01/17/2024\n",
      "Scraping data from page 1\n",
      "Scraping data from page 2\n",
      "getting data from 01/18/2024\n",
      "Scraping data from page 1\n",
      "Scraping data from page 2\n",
      "getting data from 01/19/2024\n",
      "Scraping data from page 1\n",
      "Scraping data from page 2\n",
      "Error clicking next page: Message: element click intercepted: Element <a title=\"... Page\" href=\"javascript:void(0);\" onclick=\"changePageIndex(&quot;3&quot;,&quot;ctl00_ContentPlaceHolder1_PagerControl1_hdnCurrentPage&quot;,&quot;ctl00_ContentPlaceHolder1_PagerControl1_btnPaging&quot;)\">Next</a> is not clickable at point (710, 10). Other element would receive the click: <a href=\"/....aspx\">Services</a>\n",
      "  (Session info: chrome=120.0.6099.234)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010d9c9cc8 chromedriver + 4844744\n",
      "1   chromedriver                        0x000000010d9c1243 chromedriver + 4809283\n",
      "2   chromedriver                        0x000000010d58f77d chromedriver + 411517\n",
      "3   chromedriver                        0x000000010d5e8522 chromedriver + 775458\n",
      "4   chromedriver                        0x000000010d5e5cb6 chromedriver + 765110\n",
      "5   chromedriver                        0x000000010d5e2f57 chromedriver + 753495\n",
      "6   chromedriver                        0x000000010d5e1a6b chromedriver + 748139\n",
      "7   chromedriver                        0x000000010d5d38bd chromedriver + 690365\n",
      "8   chromedriver                        0x000000010d609cc2 chromedriver + 912578\n",
      "9   chromedriver                        0x000000010d5d3118 chromedriver + 688408\n",
      "10  chromedriver                        0x000000010d60a31e chromedriver + 914206\n",
      "11  chromedriver                        0x000000010d628d42 chromedriver + 1039682\n",
      "12  chromedriver                        0x000000010d609a63 chromedriver + 911971\n",
      "13  chromedriver                        0x000000010d5d11b3 chromedriver + 680371\n",
      "14  chromedriver                        0x000000010d5d27ce chromedriver + 686030\n",
      "15  chromedriver                        0x000000010d989642 chromedriver + 4580930\n",
      "16  chromedriver                        0x000000010d98e9cc chromedriver + 4602316\n",
      "17  chromedriver                        0x000000010d96ee11 chromedriver + 4472337\n",
      "18  chromedriver                        0x000000010d98f746 chromedriver + 4605766\n",
      "19  chromedriver                        0x000000010d96039c chromedriver + 4412316\n",
      "20  chromedriver                        0x000000010d9af868 chromedriver + 4737128\n",
      "21  chromedriver                        0x000000010d9afa1e chromedriver + 4737566\n",
      "22  chromedriver                        0x000000010d9c0e83 chromedriver + 4808323\n",
      "23  libsystem_pthread.dylib             0x00007fff206718fc _pthread_start + 224\n",
      "24  libsystem_pthread.dylib             0x00007fff2066d443 thread_start + 15\n",
      "\n",
      "No more pages to scrape.\n",
      "getting data from 01/20/2024\n",
      "Scraping data from page 1\n",
      "Scraping data from page 2\n",
      "getting data from 01/21/2024\n",
      "Scraping data from page 1\n",
      "Scraping data from page 2\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the input element by its ID\n",
    "#input_element = driver.find_element_by_id('ctl00_ContentPlaceHolder1_txtFloorsheetDateFilter')\n",
    "# Initialize an empty list to store the data from all pages\n",
    "all_data = []\n",
    "for date_str in date_array:\n",
    "    try:\n",
    "\n",
    "        print(f\"getting data from {date_str}\")\n",
    "        # Find the input element by its ID\n",
    "        input_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'ctl00_ContentPlaceHolder1_txtFloorsheetDateFilter')))\n",
    "        input_element.clear()\n",
    "        input_element.send_keys(date_str)\n",
    "        # Find the span element by its ID\n",
    "        date_span = driver.find_element(By.ID, 'ctl00_ContentPlaceHolder1_marketDate')\n",
    "        # Extract the text value from the span element\n",
    "        date_value = date_span.text\n",
    "        # Print the extracted value\n",
    "        #print(\"Date Value:\", date_value)\n",
    "        # Press Enter to confirm the new date (optional, depends on the website's behavior)\n",
    "        input_element.send_keys(Keys.ENTER)\n",
    "    \n",
    "        # Wait for the element to be clickable (you might need to adjust the timeout)\n",
    "        search_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, 'ctl00_ContentPlaceHolder1_lbtnSearchFloorsheet')))\n",
    "        # Click the \"Search\" button\n",
    "        search_button.click()\n",
    "        # Extract the total pages value from the content\n",
    "        #total_pages_text = span_element.get_text(strip=True)\n",
    "        #start_index = total_pages_text.find(\"[Total pages:\") + len(\"[Total pages:\")\n",
    "        #end_index = total_pages_text.find(\"]\", start_index)\n",
    "\n",
    "        #total_pages_value = total_pages_text[start_index:end_index]\n",
    "        #num_pages_to_scrape = int(total_pages_value)\n",
    "\n",
    "        #testing number of pages:\n",
    "        num_pages_to_scrape = 2\n",
    "        # Loop through pages\n",
    "        for page_num in range(num_pages_to_scrape):\n",
    "            print(f\"Scraping data from page {page_num + 1}\")\n",
    "\n",
    "            # Extract data from the current page\n",
    "            current_data = extract_page_data()\n",
    "            all_data.extend(current_data)\n",
    "\n",
    "            # Click the next page\n",
    "            if not click_next_page():\n",
    "                print(\"No more pages to scrape.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print (f\"Error printing the page+date_str+{e}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the number of pages you want to scrape\n",
    "span_element = soup.find('span', id='ctl00_ContentPlaceHolder1_PagerControl1_litRecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the collected data\n",
    "print(\"Collected Data:\")\n",
    "for row in all_data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
